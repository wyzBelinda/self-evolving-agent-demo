{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd318ba-3612-47ed-80b4-d36810beaaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation complete.\n"
     ]
    }
   ],
   "source": [
    "# @title Step 0: Setup and Installation\n",
    "# Install ADK and LiteLLM for multi-model support\n",
    "\n",
    "# !pip install google-adk -q\n",
    "# !pip install litellm -q\n",
    "\n",
    "# !uv add google-adk -q\n",
    "# !uv add litellm -q\n",
    "\n",
    "print(\"Installation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a0d308-aa7b-4b1d-b34a-a61f0759494a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# @title Import necessary libraries\n",
    "import os\n",
    "import asyncio\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20b87f2-6545-44a9-90c9-515f580fe3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys Set:\n",
      "Google API Key set: Yes\n",
      "OpenAI API Key set: Yes\n",
      "Anthropic API Key set: No (REPLACE PLACEHOLDER!)\n",
      "DeepSeek API Key set: Yes\n"
     ]
    }
   ],
   "source": [
    "# @title Configure API Keys (Replace with your actual keys!)\n",
    "\n",
    "# --- IMPORTANT: Replace placeholders with your real API keys ---\n",
    "\n",
    "# Gemini API Key (Get from Google AI Studio: https://aistudio.google.com/app/apikey)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCFTRaeLewyHTGtJ0addZWURBBlD7fecHA\" # <--- REPLACE\n",
    "\n",
    "# [Optional]\n",
    "# OpenAI API Key (Get from OpenAI Platform: https://platform.openai.com/api-keys)\n",
    "os.environ['OPENAI_API_KEY'] = 'REDACTED' # <--- REPLACE\n",
    "\n",
    "# [Optional]\n",
    "# Anthropic API Key (Get from Anthropic Console: https://console.anthropic.com/settings/keys)\n",
    "os.environ['ANTHROPIC_API_KEY'] = 'YOUR_ANTHROPIC_API_KEY' # <--- REPLACE\n",
    "\n",
    "# [Optional]\n",
    "# DEEPSEEK API Key \n",
    "os.environ['DEEPSEEK_API_KEY'] = 'REDACTED' # <--- REPLACE\n",
    "\n",
    "\n",
    "# --- Verify Keys (Optional Check) ---\n",
    "print(\"API Keys Set:\")\n",
    "print(f\"Google API Key set: {'Yes' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "print(f\"OpenAI API Key set: {'Yes' if os.environ.get('OPENAI_API_KEY') and os.environ['OPENAI_API_KEY'] != 'REDACTED' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "print(f\"Anthropic API Key set: {'Yes' if os.environ.get('ANTHROPIC_API_KEY') and os.environ['ANTHROPIC_API_KEY'] != 'YOUR_ANTHROPIC_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "print(f\"DeepSeek API Key set: {'Yes' if os.environ.get('DEEPSEEK_API_KEY') and os.environ['DEEPSEEK_API_KEY'] != 'YOUR_DEEPSEEK_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "\n",
    "\n",
    "# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
    "\n",
    "\n",
    "# @markdown **Security Note:** It's best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Replace the placeholder strings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1353067b-b41b-44f8-a057-7bc01561332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment configured.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Model Constants for easier use ---\n",
    "\n",
    "# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\n",
    "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
    "\n",
    "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/openai#openai-chat-completion-models\n",
    "MODEL_GPT_4O = \"openai/gpt-4.1\" # You can also try: gpt-4.1-mini, gpt-4o etc.\n",
    "\n",
    "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/anthropic\n",
    "MODEL_CLAUDE_SONNET = \"anthropic/claude-sonnet-4-20250514\" # You can also try: claude-opus-4-20250514 , claude-3-7-sonnet-20250219 etc\n",
    "\n",
    "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/\n",
    "MODEL_DEEPSEEK_CHAT = \"deepseek/deepseek-chat\" \n",
    "\n",
    "print(\"\\nEnvironment configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b60c542-b3c1-43cf-8257-f4848d2a9b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool: get_weather called for city: New York ---\n",
      "{'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25Â°C.'}\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "{'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}\n"
     ]
    }
   ],
   "source": [
    "# @title Define the get_weather Tool\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Retrieves the current weather report for a specified city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the weather information.\n",
    "              Includes a 'status' key ('success' or 'error').\n",
    "              If 'success', includes a 'report' key with weather details.\n",
    "              If 'error', includes an 'error_message' key.\n",
    "    \"\"\"\n",
    "    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n",
    "    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n",
    "\n",
    "    # Mock weather data\n",
    "    mock_weather_db = {\n",
    "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25Â°C.\"},\n",
    "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15Â°C.\"},\n",
    "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18Â°C.\"},\n",
    "    }\n",
    "\n",
    "    if city_normalized in mock_weather_db:\n",
    "        return mock_weather_db[city_normalized]\n",
    "    else:\n",
    "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
    "\n",
    "# Example tool usage (optional test)\n",
    "print(get_weather(\"New York\"))\n",
    "print(get_weather(\"Paris\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ccabc81-a20c-490a-a47c-5f101692a6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'weather_agent_v1' created using model 'gemini-2.0-flash'.\n"
     ]
    }
   ],
   "source": [
    "# @title Define the Weather Agent\n",
    "# Use one of the model constants defined earlier\n",
    "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"weather_agent_v1\",\n",
    "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
    "    description=\"Provides weather information for specific cities.\",\n",
    "    instruction=\"You are a helpful weather assistant. \"\n",
    "                \"When the user asks for the weather in a specific city, \"\n",
    "                \"use the 'get_weather' tool to find the information. \"\n",
    "                \"If the tool returns an error, inform the user politely. \"\n",
    "                \"If the tool is successful, present the weather report clearly.\",\n",
    "    tools=[get_weather], # Pass the function directly\n",
    ")\n",
    "\n",
    "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d565a32a-cb15-4de2-bf7d-3be6fc84a539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created: App='weather_tutorial_app', User='user_1', Session='session_001'\n",
      "Runner created for agent 'weather_agent_v1'.\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Session Service and Runner\n",
    "\n",
    "# --- Session Management ---\n",
    "# Key Concept: SessionService stores conversation history & state.\n",
    "# InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Define constants for identifying the interaction context\n",
    "APP_NAME = \"weather_tutorial_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n",
    "\n",
    "# Create the specific session where the conversation will happen\n",
    "session = await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID\n",
    ")\n",
    "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
    "\n",
    "# --- Runner ---\n",
    "# Key Concept: Runner orchestrates the agent execution loop.\n",
    "runner = Runner(\n",
    "    agent=weather_agent, # The agent we want to run\n",
    "    app_name=APP_NAME,   # Associates runs with our app\n",
    "    session_service=session_service # Uses our session manager\n",
    ")\n",
    "print(f\"Runner created for agent '{runner.agent.name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e89dcc-20a8-45f8-8509-e3340dd3741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Agent Interaction Function\n",
    "\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "async def call_agent_async(query: str, runner, user_id, session_id):\n",
    "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
    "  print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "  # Prepare the user's message in ADK format\n",
    "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "\n",
    "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
    "\n",
    "  # Key Concept: run_async executes the agent logic and yields Events.\n",
    "  # We iterate through events to find the final answer.\n",
    "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
    "      # You can uncomment the line below to see *all* events during execution\n",
    "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "\n",
    "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
    "      if event.is_final_response():\n",
    "          if event.content and event.content.parts:\n",
    "             # Assuming text response in the first part\n",
    "             final_response_text = event.content.parts[0].text\n",
    "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
    "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "          # Add more checks here if needed (e.g., specific error codes)\n",
    "          break # Stop processing events once the final response is found\n",
    "\n",
    "  print(f\"<<< Agent Response: {final_response_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c649ff6-5ac7-4679-9c12-7fa9b4c5dbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: What is the weather like in London?\n",
      "--- Tool: get_weather called for city: London ---\n",
      "<<< Agent Response: The weather in New York is sunny with a temperature of 25Â°C.\n",
      "\n",
      "\n",
      ">>> User Query: How about Paris?\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "<<< Agent Response: I am sorry, I don't have weather information for Paris.\n",
      "\n",
      "\n",
      ">>> User Query: Tell me the weather in New York\n",
      "--- Tool: get_weather called for city: New York ---\n",
      "<<< Agent Response: The weather in New York is sunny with a temperature of 25Â°C.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Run the Initial Conversation\n",
    "\n",
    "# We need an async function to await our interaction helper\n",
    "async def run_conversation():\n",
    "    await call_agent_async(\"What is the weather like in London?\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID)\n",
    "\n",
    "    await call_agent_async(\"How about Paris?\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID) # Expecting the tool's error message\n",
    "\n",
    "    await call_agent_async(\"Tell me the weather in New York\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID)\n",
    "\n",
    "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
    "await run_conversation()\n",
    "\n",
    "# --- OR ---\n",
    "\n",
    "# Uncomment the following lines if running as a standard Python script (.py file):\n",
    "# import asyncio\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         asyncio.run(run_conversation())\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d58ef-8116-46c1-92c4-990e4c0830a5",
   "metadata": {},
   "source": [
    "# Step 2: Going Multi-Model with LiteLLM [Optional]Â¶\n",
    "ç¬¬ 2 æ­¥ï¼šä½¿ç”¨ LiteLLM è¿›è¡Œå¤šæ¨¡åž‹ [å¯é€‰]Â¶\n",
    "In Step 1, we built a functional Weather Agent powered by a specific Gemini model. While effective, real-world applications often benefit from the flexibility to use different Large Language Models (LLMs). Why?\n",
    "åœ¨ç¬¬ 1 æ­¥ä¸­ï¼Œæˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªç”±ç‰¹å®š Gemini æ¨¡åž‹æä¾›æ”¯æŒçš„åŠŸèƒ½æ€§å¤©æ°”ä»£ç†ã€‚è™½ç„¶æœ‰æ•ˆï¼Œä½†å®žé™…åº”ç”¨ç¨‹åºé€šå¸¸å—ç›ŠäºŽä½¿ç”¨ä¸åŒå¤§åž‹è¯­è¨€æ¨¡åž‹ ï¼ˆLLMï¼‰ çš„çµæ´»æ€§ã€‚ä¸ºä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "Performance: Some models excel at specific tasks (e.g., coding, reasoning, creative writing).\n",
    "æ€§èƒ½ï¼š æœ‰äº›æ¨¡åž‹æ“…é•¿ç‰¹å®šä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œç¼–ç ã€æŽ¨ç†ã€åˆ›æ„å†™ä½œï¼‰ã€‚\n",
    "Cost: Different models have varying price points.\n",
    "æˆæœ¬ï¼š ä¸åŒçš„åž‹å·æœ‰ä¸åŒçš„ä»·ä½ã€‚\n",
    "Capabilities: Models offer diverse features, context window sizes, and fine-tuning options.\n",
    "èƒ½åŠ›ï¼š æ¨¡åž‹æä¾›å¤šç§åŠŸèƒ½ã€ä¸Šä¸‹æ–‡çª—å£å¤§å°å’Œå¾®è°ƒé€‰é¡¹ã€‚\n",
    "Availability/Redundancy: Having alternatives ensures your application remains functional even if one provider experiences issues.\n",
    "å¯ç”¨æ€§/å†—ä½™ï¼š æ‹¥æœ‰æ›¿ä»£æ–¹æ¡ˆå¯ç¡®ä¿æ‚¨çš„åº”ç”¨ç¨‹åºä¿æŒæ­£å¸¸è¿è¡Œï¼Œå³ä½¿ä¸€ä¸ªæä¾›å•†é‡åˆ°é—®é¢˜ã€‚\n",
    "ADK makes switching between models seamless through its integration with the LiteLLM library. LiteLLM acts as a consistent interface to over 100 different LLMs.\n",
    "ADK é€šè¿‡ä¸Ž LiteLLM åº“çš„é›†æˆï¼Œå®žçŽ°æ¨¡åž‹ä¹‹é—´çš„æ— ç¼åˆ‡æ¢ã€‚LiteLLM å……å½“ 100 å¤šç§ä¸åŒ LLM çš„ä¸€è‡´æŽ¥å£ã€‚\n",
    "\n",
    "In this step, we will:\n",
    "åœ¨æ­¤æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š\n",
    "\n",
    "Learn how to configure an ADK Agent to use models from providers like OpenAI (GPT) and Anthropic (Claude) using the LiteLlm wrapper.\n",
    "äº†è§£å¦‚ä½•ä½¿ç”¨ LiteLlm åŒ…è£…å™¨é…ç½® ADK ä»£ç†ä»¥ä½¿ç”¨ OpenAI ï¼ˆGPTï¼‰ å’Œ Anthropic ï¼ˆClaudeï¼‰ ç­‰æä¾›å•†çš„æ¨¡åž‹ã€‚\n",
    "Define, configure (with their own sessions and runners), and immediately test instances of our Weather Agent, each backed by a different LLM.\n",
    "å®šä¹‰ã€é…ç½®ï¼ˆä½¿ç”¨è‡ªå·±çš„ä¼šè¯å’Œè¿è¡Œå™¨ï¼‰å¹¶ç«‹å³æµ‹è¯•æˆ‘ä»¬çš„å¤©æ°”ä»£ç†å®žä¾‹ï¼Œæ¯ä¸ªå®žä¾‹éƒ½ç”±ä¸åŒçš„ LLM æ”¯æŒã€‚\n",
    "Interact with these different agents to observe potential variations in their responses, even when using the same underlying tool.\n",
    "ä¸Žè¿™äº›ä¸åŒçš„ä»£ç†äº¤äº’ä»¥è§‚å¯Ÿå…¶ååº”çš„æ½œåœ¨å˜åŒ–ï¼Œå³ä½¿ä½¿ç”¨ç›¸åŒçš„åº•å±‚å·¥å…·ä¹Ÿæ˜¯å¦‚æ­¤ã€‚\n",
    "## 1. Import LiteLlm\n",
    "\n",
    "\n",
    "We imported this during the initial setup (Step 0), but it's the key component for multi-model support:\n",
    "æˆ‘ä»¬åœ¨åˆå§‹è®¾ç½®ï¼ˆæ­¥éª¤ 0ï¼‰æœŸé—´å¯¼å…¥äº†å®ƒï¼Œä½†å®ƒæ˜¯å¤šæ¨¡åž‹æ”¯æŒçš„å…³é”®ç»„ä»¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fe58da9-e29e-4206-82fd-0c5b3b4d2403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Import LiteLlm\n",
    "from google.adk.models.lite_llm import LiteLlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7342f58e-3175-4f4c-8e1d-48f690b2bb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'weather_agent_gpt' created using model 'deepseek/deepseek-chat'.\n",
      "Session created: App='weather_tutorial_app_gpt', User='user_1_gpt', Session='session_001_gpt'\n",
      "Runner created for agent 'weather_agent_gpt'.\n",
      "\n",
      "--- Testing GPT Agent ---\n",
      "\n",
      ">>> User Query: What's the weather in Tokyo?\n",
      "--- Tool: get_weather called for city: Tokyo ---\n",
      "<<< Agent Response: Here's the current weather in Tokyo:\n",
      "\n",
      "ðŸŒ§ï¸ **Weather**: Light rain  \n",
      "ðŸŒ¡ï¸ **Temperature**: 18Â°C (64Â°F)\n",
      "\n",
      "It looks like it's a bit rainy in Tokyo right now with mild temperatures. You might want to bring an umbrella if you're heading out!\n"
     ]
    }
   ],
   "source": [
    "# @title Define and Test GPT Agent\n",
    "\n",
    "# Make sure 'get_weather' function from Step 1 is defined in your environment.\n",
    "# Make sure 'call_agent_async' is defined from earlier.\n",
    "\n",
    "# --- Agent using GPT-4o ---\n",
    "weather_agent_gpt = None # Initialize to None\n",
    "runner_gpt = None      # Initialize runner to None\n",
    "\n",
    "try:\n",
    "    weather_agent_gpt = Agent(\n",
    "        name=\"weather_agent_gpt\",\n",
    "        # Key change: Wrap the LiteLLM model identifier\n",
    "        # model=LiteLlm(model=MODEL_GPT_4O),\n",
    "        model=LiteLlm(model=MODEL_DEEPSEEK_CHAT),\n",
    "        description=\"Provides weather information (using DeepSeek).\",\n",
    "        instruction=\"You are a helpful weather assistant powered by DeepSeek. \"\n",
    "                    \"Use the 'get_weather' tool for city weather requests. \"\n",
    "                    \"Clearly present successful reports or polite error messages based on the tool's output status.\",\n",
    "        tools=[get_weather], # Re-use the same tool\n",
    "    )\n",
    "    print(f\"Agent '{weather_agent_gpt.name}' created using model '{MODEL_DEEPSEEK_CHAT}'.\")\n",
    "\n",
    "    # InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "    session_service_gpt = InMemorySessionService() # Create a dedicated service\n",
    "\n",
    "    # Define constants for identifying the interaction context\n",
    "    APP_NAME_GPT = \"weather_tutorial_app_gpt\" # Unique app name for this test\n",
    "    USER_ID_GPT = \"user_1_gpt\"\n",
    "    SESSION_ID_GPT = \"session_001_gpt\" # Using a fixed ID for simplicity\n",
    "\n",
    "    # Create the specific session where the conversation will happen\n",
    "    session_gpt = await session_service_gpt.create_session(\n",
    "        app_name=APP_NAME_GPT,\n",
    "        user_id=USER_ID_GPT,\n",
    "        session_id=SESSION_ID_GPT\n",
    "    )\n",
    "    print(f\"Session created: App='{APP_NAME_GPT}', User='{USER_ID_GPT}', Session='{SESSION_ID_GPT}'\")\n",
    "\n",
    "    # Create a runner specific to this agent and its session service\n",
    "    runner_gpt = Runner(\n",
    "        agent=weather_agent_gpt,\n",
    "        app_name=APP_NAME_GPT,       # Use the specific app name\n",
    "        session_service=session_service_gpt # Use the specific session service\n",
    "        )\n",
    "    print(f\"Runner created for agent '{runner_gpt.agent.name}'.\")\n",
    "\n",
    "    # --- Test the GPT Agent ---\n",
    "    print(\"\\n--- Testing GPT Agent ---\")\n",
    "    # Ensure call_agent_async uses the correct runner, user_id, session_id\n",
    "    await call_agent_async(query = \"What's the weather in Tokyo?\",\n",
    "                           runner=runner_gpt,\n",
    "                           user_id=USER_ID_GPT,\n",
    "                           session_id=SESSION_ID_GPT)\n",
    "    # --- OR ---\n",
    "\n",
    "    # Uncomment the following lines if running as a standard Python script (.py file):\n",
    "    # import asyncio\n",
    "    # if __name__ == \"__main__\":\n",
    "    #     try:\n",
    "    #         asyncio.run(call_agent_async(query = \"What's the weather in Tokyo?\",\n",
    "    #                      runner=runner_gpt,\n",
    "    #                       user_id=USER_ID_GPT,\n",
    "    #                       session_id=SESSION_ID_GPT)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"An error occurred: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not create or run GPT agent '{MODEL_GPT_4O}'. Check API Key and model name. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89565fed-202f-4444-b04a-eb85947fbe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define and Test Claude Agent\n",
    "\n",
    "# Make sure 'get_weather' function from Step 1 is defined in your environment.\n",
    "# Make sure 'call_agent_async' is defined from earlier.\n",
    "\n",
    "# --- Agent using Claude Sonnet ---\n",
    "weather_agent_claude = None # Initialize to None\n",
    "runner_claude = None      # Initialize runner to None\n",
    "\n",
    "try:\n",
    "    weather_agent_claude = Agent(\n",
    "        name=\"weather_agent_claude\",\n",
    "        # Key change: Wrap the LiteLLM model identifier\n",
    "        model=LiteLlm(model=MODEL_CLAUDE_SONNET),\n",
    "        description=\"Provides weather information (using Claude Sonnet).\",\n",
    "        instruction=\"You are a helpful weather assistant powered by Claude Sonnet. \"\n",
    "                    \"Use the 'get_weather' tool for city weather requests. \"\n",
    "                    \"Analyze the tool's dictionary output ('status', 'report'/'error_message'). \"\n",
    "                    \"Clearly present successful reports or polite error messages.\",\n",
    "        tools=[get_weather], # Re-use the same tool\n",
    "    )\n",
    "    print(f\"Agent '{weather_agent_claude.name}' created using model '{MODEL_CLAUDE_SONNET}'.\")\n",
    "\n",
    "    # InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "    session_service_claude = InMemorySessionService() # Create a dedicated service\n",
    "\n",
    "    # Define constants for identifying the interaction context\n",
    "    APP_NAME_CLAUDE = \"weather_tutorial_app_claude\" # Unique app name\n",
    "    USER_ID_CLAUDE = \"user_1_claude\"\n",
    "    SESSION_ID_CLAUDE = \"session_001_claude\" # Using a fixed ID for simplicity\n",
    "\n",
    "    # Create the specific session where the conversation will happen\n",
    "    session_claude = await session_service_claude.create_session(\n",
    "        app_name=APP_NAME_CLAUDE,\n",
    "        user_id=USER_ID_CLAUDE,\n",
    "        session_id=SESSION_ID_CLAUDE\n",
    "    )\n",
    "    print(f\"Session created: App='{APP_NAME_CLAUDE}', User='{USER_ID_CLAUDE}', Session='{SESSION_ID_CLAUDE}'\")\n",
    "\n",
    "    # Create a runner specific to this agent and its session service\n",
    "    runner_claude = Runner(\n",
    "        agent=weather_agent_claude,\n",
    "        app_name=APP_NAME_CLAUDE,       # Use the specific app name\n",
    "        session_service=session_service_claude # Use the specific session service\n",
    "        )\n",
    "    print(f\"Runner created for agent '{runner_claude.agent.name}'.\")\n",
    "\n",
    "    # --- Test the Claude Agent ---\n",
    "    print(\"\\n--- Testing Claude Agent ---\")\n",
    "    # Ensure call_agent_async uses the correct runner, user_id, session_id\n",
    "    await call_agent_async(query = \"Weather in London please.\",\n",
    "                           runner=runner_claude,\n",
    "                           user_id=USER_ID_CLAUDE,\n",
    "                           session_id=SESSION_ID_CLAUDE)\n",
    "\n",
    "    # --- OR ---\n",
    "\n",
    "    # Uncomment the following lines if running as a standard Python script (.py file):\n",
    "    # import asyncio\n",
    "    # if __name__ == \"__main__\":\n",
    "    #     try:\n",
    "    #         asyncio.run(call_agent_async(query = \"Weather in London please.\",\n",
    "    #                      runner=runner_claude,\n",
    "    #                       user_id=USER_ID_CLAUDE,\n",
    "    #                       session_id=SESSION_ID_CLAUDE)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not create or run Claude agent '{MODEL_CLAUDE_SONNET}'. Check API Key and model name. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
